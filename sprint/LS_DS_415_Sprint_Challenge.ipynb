{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import langid\n",
    "from googletrans import Translator\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_word(token):\n",
    "    rules = set([\n",
    "        token.is_punct,\n",
    "        token.pos_ == 'SPACE',\n",
    "        token.pos_ == 'PRON',\n",
    "        token.pos_ == 'DET',\n",
    "    ])\n",
    "    \n",
    "    return True not in rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    return [token.lemma_ for token in nlp(doc) if validate_word(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jump', 'over', 'lazy', 'dog']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(u\"The quick brown fox jumps over the lazy dog.  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.5: Detect Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = yelp['text']\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['lang'] = reviews.apply(langid.classify).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    9994\n",
       "lv       2\n",
       "es       2\n",
       "id       1\n",
       "la       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_en = yelp['lang'] != 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_en_values = yelp[non_en]['text'].str[:1500].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "translations = translator.translate(non_en_values, dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp.loc[non_en, 'text'] = [translation.text for translation in translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = yelp['text']\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['lang'] = reviews.apply(langid.classify).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    9994\n",
       "lv       2\n",
       "es       2\n",
       "id       1\n",
       "la       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = reviews.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [beware, fake, fake, fake, also, own, small, b...\n",
       "1     [come, here, for, lunch, Togo, service, be, qu...\n",
       "2     [have, be, to, Vegas, dozen, of, time, and, ha...\n",
       "3     [go, here, on, night, where, close, off, part,...\n",
       "4     [3.5, to, 4, star, not, bad, for, price, $, 12...\n",
       "5     [tasty, fast, casual, latin, street, food, men...\n",
       "6     [show, be, absolutely, amazing, incredible, pr...\n",
       "7     [come, for, Pho, and, really, enjoy, get, here...\n",
       "8     [absolutely, most, unique, experience, in, nai...\n",
       "9     [wow, walk, in, and, sit, at, bar, for, 10, mi...\n",
       "10    [pop, in, for, dinner, yesterday, with, reserv...\n",
       "11    [bad, stay, ever, so, first, end, up, pay, ove...\n",
       "12    [great, friendly, customer, service, and, qual...\n",
       "13    [food, be, great, be, super, busy, but, server...\n",
       "14    [talk, about, get, rip, off, charge, $, 420, f...\n",
       "15    [girl, night, out, tonight, with, kid, so, dec...\n",
       "16    [stop, in, here, for, few, drink, fly, out, of...\n",
       "17    [be, excellent, restaurant, and, encourage, an...\n",
       "18    [have, purchase, Groupon, for, massage, from, ...\n",
       "19    [below, be, previous, review, and, still, stan...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "X = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 23022)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\"eat</th>\n",
       "      <th>\"hair</th>\n",
       "      <th>\"was</th>\n",
       "      <th>\"well</th>\n",
       "      <th>$</th>\n",
       "      <th>$2</th>\n",
       "      <th>$35</th>\n",
       "      <th>$5</th>\n",
       "      <th>'</th>\n",
       "      <th>'d</th>\n",
       "      <th>...</th>\n",
       "      <th>„Å£</th>\n",
       "      <th>„Åæ</th>\n",
       "      <th>„ÇÉ</th>\n",
       "      <th>„Çâ</th>\n",
       "      <th>‰æÜ‰∏ÄÂÄãÁÜ±Ëæ£Ëæ£ÁâõËÇâÁ≤âÔºåÊªøË∂≥</th>\n",
       "      <th>Âè∞ÊπæÈ∏°Êéí</th>\n",
       "      <th>ÂêÉ‰∫ÜÂ§™Â§ötim</th>\n",
       "      <th>ÊºÅËÜ≥Êàø</th>\n",
       "      <th>ÁæéÂë≥ÁöÑÂë≥ÈÅì</th>\n",
       "      <th>Ë±ÜËÖêËä±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23022 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   \"eat  \"hair  \"was  \"well         $   $2  $35   $5    '   'd  ...    „Å£    „Åæ  \\\n",
       "0   0.0    0.0   0.0    0.0  0.060491  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1   0.0    0.0   0.0    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2   0.0    0.0   0.0    0.0  0.081615  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3   0.0    0.0   0.0    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4   0.0    0.0   0.0    0.0  0.076258  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "     „ÇÉ    „Çâ  ‰æÜ‰∏ÄÂÄãÁÜ±Ëæ£Ëæ£ÁâõËÇâÁ≤âÔºåÊªøË∂≥  Âè∞ÊπæÈ∏°Êéí  ÂêÉ‰∫ÜÂ§™Â§ötim  ÊºÅËÜ≥Êàø  ÁæéÂë≥ÁöÑÂë≥ÈÅì  Ë±ÜËÖêËä±  \n",
       "0  0.0  0.0           0.0   0.0      0.0  0.0    0.0  0.0  \n",
       "1  0.0  0.0           0.0   0.0      0.0  0.0    0.0  0.0  \n",
       "2  0.0  0.0           0.0   0.0      0.0  0.0    0.0  0.0  \n",
       "3  0.0  0.0           0.0   0.0      0.0  0.0    0.0  0.0  \n",
       "4  0.0  0.0           0.0   0.0      0.0  0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 23022 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "print(dtm.shape)\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "nn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = tokenize(\"Not the worst chinese food I've ever had. Forgot my fortune cookie.\")\n",
    "fake_review_vectorize = vectorizer.transform(fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.19006405, 1.19821941, 1.19852671, 1.19957979, 1.20184542,\n",
       "         1.21323596, 1.21335749, 1.22101066, 1.22294512, 1.22881943],\n",
       "        [0.79027074, 0.96005179, 1.01531882, 1.14849721, 1.1502259 ,\n",
       "         1.17037217, 1.18080038, 1.18856006, 1.19919452, 1.20437573],\n",
       "        [0.90218501, 0.94229529, 0.96870488, 1.01151261, 1.02513431,\n",
       "         1.03706399, 1.04820767, 1.05968911, 1.09470385, 1.09881138],\n",
       "        [1.10308972, 1.1200234 , 1.12245789, 1.12421956, 1.14637019,\n",
       "         1.15120363, 1.15383282, 1.17548761, 1.18881519, 1.18957245],\n",
       "        [1.09071518, 1.13800907, 1.15779532, 1.16888502, 1.17186158,\n",
       "         1.17723281, 1.18352129, 1.18564181, 1.18889797, 1.18973561],\n",
       "        [1.12000772, 1.12783248, 1.14500098, 1.16093364, 1.1724754 ,\n",
       "         1.19327466, 1.19679312, 1.19770837, 1.2041125 , 1.22107604],\n",
       "        [1.09071518, 1.13800907, 1.15779532, 1.16888502, 1.17186158,\n",
       "         1.17723281, 1.18352129, 1.18564181, 1.18889797, 1.18973561],\n",
       "        [1.16117518, 1.16951688, 1.17290927, 1.18698871, 1.19906821,\n",
       "         1.21742652, 1.22659602, 1.23072951, 1.23543805, 1.24607935],\n",
       "        [1.23401712, 1.24796161, 1.26246152, 1.27441503, 1.303923  ,\n",
       "         1.31220846, 1.33354816, 1.3494923 , 1.35199709, 1.37829922],\n",
       "        [0.84150052, 0.96350325, 1.05692297, 1.11533868, 1.11704929,\n",
       "         1.13242799, 1.1325049 , 1.13433296, 1.15726669, 1.16075525]]),\n",
       " array([[2501, 4623, 9776, 2516, 2369, 5124, 5356, 8913, 5441,  233],\n",
       "        [4599, 2518, 8614, 9056, 5164, 8134, 2990, 6847,  417, 7579],\n",
       "        [8647, 6429, 7611,  151, 6610, 6156, 6522, 4722,  562,  409],\n",
       "        [ 151, 5267, 9355, 7351, 6570, 7949, 7953, 2995,   47, 9720],\n",
       "        [7908, 4403, 7391, 1343, 3121, 2633, 5983, 7078, 6060,  697],\n",
       "        [2971, 5983,  281,  207, 8732,  869, 2529, 2349, 4475, 6316],\n",
       "        [7908, 4403, 7391, 1343, 3121, 2633, 5983, 7078, 6060,  697],\n",
       "        [2993, 6229, 1298, 2867, 2042, 6873, 1245, 6594, 6514, 6177],\n",
       "        [1467, 1970, 9010, 2611, 3110, 6098, 9198, 1586, 4219, 5273],\n",
       "        [9692, 8219,  406, 7987, 9796,  400,  273, 9632, 9975, 6489]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = nn.kneighbors(fake_review_vectorize.todense())\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"We really wanted to like this theater but it didn't happen.  The service was not good and the food was cold when it arrived.  Very disappointing and we will not be returning.\",\n",
       "       \"For the people that said service was Excellent, I question your opinion in Excellence.\\n   The food here is Pretty good if you're really into sushi. The Vampire roll is my fave and has garlic ponzu sauce on top, they have Great Udon and Miso. LOVE their Poke and seaweed salad and they have a few other good ones too AND out of the 6 times I've gone, I never had to wait for a table. \\n   BIG But however, Absolutely Trust in these words and the others that say the service is not good and the woman that are waiting on you are mostly standing in the back doing nothing. I have not sat at the bar yet.  \\n   \\n   If you have somewhere to be, do not come here. It's the slowest sushi restaurant I've been to in LV and I've been to at least 10 in this city. Look forward to them bringing someone else's order to your table, not checking up on you for water until after it's gone (not every time but enough times to be parched), they take your AYCE card away and don't return it so you can order more; you have to ask for it back. And thats about it. They're pleasant though, not mean at all, they just don't have things very organized and don't tend to you as they should.  \\n\\n  Great food, bad service. I have not been there once out of 6 times and the service changed. Literally. I don't just bash restaurants and I hope you see I'm not doing that. I'm not hard to please and LOVE sushi. I wish SO much the service was better b.c the food really is good, it's just not here. Thats the only reason for the 3 stars, the food. So, go for the food and if you're hungry, expect to be there for a bit.\",\n",
       "       'The food here is not very good.  The service is slow.   It is a bit over price.   I would not recommend this place.',\n",
       "       'Don\\'t bother wasting your money here. I had a gift certificate for mani/pedi. I was able to get helped as a walk in so that was good. The rest wasn\\'t so. They do everything very quickly and not with much care or attention. As soon as I drove off, I noticed what a terrible job they had done on my hands. None of my cuticles were pushed back or any dead skin clipped off. I could have done a better job at home. \\n\\nI went back to speak to the manager and she was not courteous and pretty rude. She refused to accept that it wasn\\'t done well and just kept arguing and speaking to her coworkers in chinese. I gave up and said I\\'ll leave but won\\'t come back to this place. Her response to me \"yeah go don\\'t come back\". This obviously doesn\\'t care to lose business from clients and I don\\'t think this seems new to her with such an unprofessional attitude.\\n\\nThis place isn\\'t cheaper than any other place and by far the worst I\\'ve tried in the city so far.\\n\\nThey\\'re so bad they don\\'t even send you out with dry nail polish. Mine just got destroyed as soon we I left.',\n",
       "       'The girl who gave me the mani/pedi was nice, she did a great job.\\nI just am not impressed with what have been a $50 service. The pedicure chairs are uncomfortable, they also don\\'t have a massage feature. I wasn\\'t asked what temperature I want the water before they filled up the bowl. I wasn\\'t asked what pressure I like for the leg or hand massage.\\nThe \"nail bar\" area is fine- also not super comfortable.\\nNo WiFi capabilities. \\nNot sure what they were going for with decor, it\\'s \"interesting\"\\n\\nBottom line: will not return.',\n",
       "       \"Why am I giving it a one-star review? Isn't the food any good? I don't know, because the place isn't there! I can confirm that it isn't (reliably) at the location given during the hours specified. That makes this Yelp entry worse than worthless; it's misleading. If you're hungry and this comes up, don't drive there thinking you're going to find it.\",\n",
       "       \"Definitely not the best Italian restaurant I've been to. They weren't very busy in a Monday night. The waitress was friendly and quick another staff member made sure our water never got below halfway. We had calamari for the appetizer and it was ok, kinda chewy. I had the seafood Alfredo and it wasn't the best, but not the worst. I wouldn't come here again by myself but I would go if others were going.\",\n",
       "       'Absolutely great burgers. Onion rings were good.  Fries were not great, not sure how you can not make great fries. Love the condiment bar.',\n",
       "       \"Update your hours. POS. Shouldn't say you're open till 11 when everything on your door says you're not. Won't come back\",\n",
       "       'Wings are fine, better from most of the places. Other locations-way better then this one. Just look like a fast food- not even a bar. Nice people working there, but service just not right - fun, but not right. One of the girls had a shot with customers on the next table. Not professional at all.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[neighbors[1][0]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spacy takes so long, i'm not going to use a pipeline, that will have to refit that every time, would take hours... sue me lol ü§∑‚Äç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = yelp['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6084848484848485"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'penalty': ('l1', 'l2'),\n",
    "    'loss': ('hinge', 'squared_hinge'),\n",
    "    'C': (0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(clf, params, n_jobs = -1, cv=5, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:   24.5s\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:   28.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                                 fit_intercept=True, intercept_scaling=1,\n",
       "                                 loss='squared_hinge', max_iter=1000,\n",
       "                                 multi_class='ovr', penalty='l2',\n",
       "                                 random_state=42, tol=0.0001, verbose=0),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': (0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95),\n",
       "                         'loss': ('hinge', 'squared_hinge'),\n",
       "                         'penalty': ('l1', 'l2')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222388059701492"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.2, 'loss': 'squared_hinge', 'penalty': 'l2'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.18.1'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(tokens.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in tokens.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=5,\n",
    "                   workers=4,\n",
    "                   num_topics = 10 # You can change this parameter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.059*\"be\" + 0.048*\"and\" + 0.030*\"to\" + 0.020*\"have\" + 0.016*\"not\" + 0.015*\"for\" + 0.014*\"of\" + 0.012*\"with\" + 0.011*\"in\" + 0.010*\"do\"'),\n",
       " (1,\n",
       "  '0.054*\"be\" + 0.042*\"and\" + 0.027*\"to\" + 0.021*\"of\" + 0.016*\"in\" + 0.016*\"have\" + 0.015*\"not\" + 0.014*\"for\" + 0.011*\"but\" + 0.009*\"with\"'),\n",
       " (2,\n",
       "  '0.063*\"be\" + 0.044*\"and\" + 0.024*\"of\" + 0.024*\"to\" + 0.019*\"have\" + 0.018*\"in\" + 0.014*\"not\" + 0.014*\"for\" + 0.010*\"with\" + 0.010*\"but\"'),\n",
       " (3,\n",
       "  '0.052*\"be\" + 0.047*\"and\" + 0.037*\"to\" + 0.021*\"have\" + 0.018*\"for\" + 0.014*\"in\" + 0.013*\"not\" + 0.012*\"of\" + 0.010*\"with\" + 0.010*\"on\"'),\n",
       " (4,\n",
       "  '0.063*\"be\" + 0.041*\"and\" + 0.028*\"to\" + 0.021*\"not\" + 0.018*\"of\" + 0.017*\"have\" + 0.015*\"do\" + 0.013*\"for\" + 0.012*\"in\" + 0.010*\"with\"'),\n",
       " (5,\n",
       "  '0.066*\"be\" + 0.039*\"and\" + 0.035*\"to\" + 0.022*\"have\" + 0.018*\"not\" + 0.018*\"of\" + 0.012*\"in\" + 0.011*\"for\" + 0.010*\"on\" + 0.009*\"with\"'),\n",
       " (6,\n",
       "  '0.076*\"be\" + 0.040*\"and\" + 0.027*\"have\" + 0.026*\"to\" + 0.017*\"of\" + 0.016*\"for\" + 0.015*\"not\" + 0.012*\"with\" + 0.010*\"in\" + 0.008*\"but\"'),\n",
       " (7,\n",
       "  '0.073*\"be\" + 0.043*\"and\" + 0.025*\"to\" + 0.019*\"of\" + 0.017*\"not\" + 0.017*\"in\" + 0.016*\"for\" + 0.011*\"have\" + 0.009*\"on\" + 0.009*\"with\"'),\n",
       " (8,\n",
       "  '0.066*\"be\" + 0.037*\"and\" + 0.032*\"to\" + 0.023*\"have\" + 0.015*\"in\" + 0.014*\"not\" + 0.014*\"of\" + 0.014*\"for\" + 0.011*\"with\" + 0.010*\"but\"'),\n",
       " (9,\n",
       "  '0.085*\"be\" + 0.036*\"and\" + 0.035*\"to\" + 0.018*\"of\" + 0.014*\"have\" + 0.014*\"in\" + 0.013*\"not\" + 0.013*\"for\" + 0.010*\"but\" + 0.008*\"with\"')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda.print_topics()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [' '.join(t[0:5]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be and to have not',\n",
       " 'be and to of in',\n",
       " 'be and of to have',\n",
       " 'be and to have for',\n",
       " 'be and to not of',\n",
       " 'be and to have not',\n",
       " 'be and have to of',\n",
       " 'be and to of not',\n",
       " 'be and to have in',\n",
       " 'be and to of have']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "lambda",
   "language": "python",
   "name": "lambda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
